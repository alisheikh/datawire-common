.. _barker_tutorial:

Introduction to Barker
#####################

Barker is a set of microservices that mimics sending a tweet ("bark")
to a set of followers. Barker is intended to illustrate how to design
and connect a set of microservices using Datawire.

Barker Quickstart
=================

Barker includes a launch script that will start all the Barker
microservices::

  cd barker
  python launch.py

In addition to some diagnostic output, you'll see a stream of barks
continuously scroll by; these are randomly generated by the
``autobark.py`` client. The launch script includes a call to the
``listen.py`` client displaying barks directed to the user ``ark3``. You
can send barks using the ``bark.py`` client in another terminal::

  cd barker
  python bark.py ark3 This is my first manual bark.

Visit ``barker/webui/index.html`` in your browser to see the Barker UI.
A ``file://`` URL will work in most cases; see :ref:`websockets-notes`
for more information. You will see the same stream of barks for user
``ark3``, but with the newest barks on top. You can type a bark into the
UI and you'll see it appear in the stream of barks.

Barker also includes a basic monitoring setup. You can start the
monitoring with the monitoring launch script::

  cd monitoring
  python launch.py

Visit ``monitoring/index.html`` in your browser and you'll see two
real time graphs giving visibility into the state of the system.

.. note:: Killing a launch script (Ctrl-C) will kill all the
          microservices launched by that script.

Architecture
============

The architectural design of Barker is representative of many typical
processing problems. There is some large amount of input that needs to
be processed, one or more pieces of core business logic that implement
the desired processing, and some large amount of output that needs to be
collected and distributed. The overall structure of Barker's
architecture resembles the `LMAX architecture
<http://martinfowler.com/articles/lmax.html>`_.

.. image:: barker-arch.png
   :width: 100%

The nature of the microservices comprising Barker was influenced by this
`article about Twitter scalability
<http://highscalability.com/blog/2013/7/8/the-architecture-twitter-uses-to-deal-with-150m-active-users.html>`_.
Twitter refers to new messages entering the system as residing in the
users' outboxes; messages awaiting retrieval for consumption reside in
users' inboxes. With the inbox/outbox manifolds keeping track of all the
messages in play, the clients that submit and retrieve messages can be
simple and stateless, like the Barker Web UI. The business logic in the
middle only has to determine which inboxes each message needs to reach.

One can imagine additional business logic sitting between the outboxes
and inboxes, functioning much the same way. A user followers service
could pay attention to *follow* and *unfollow* messages from the
clients, using them to keep track of who follows whom. A follow
suggestions service could use the followers graph to suggest additional
users to follow, passing said suggestions to the inboxes as messages.

Barker Slow Start
=================

Now that you have a grasp of the Barker architecture, let's go through
Barker in more detail. The launch script starts a number of different
microservices locally, each operating on a different host port. The main
services are the following:

* directory, which provides service location functionality
* a manifold that functions as the user inboxes
* an instance of business logic
* three manifold instances that function as the user outboxes
* five autobark instances that simulate multiple tweeters

We've instantiated different numbers of microservices to demonstrate
several different routing algorithms. For the outbox and inbox
manifolds, we use a consistent hashing algorithm that distributes
connections between different instances based on the username. In the
bizlogic, we use an ordered algorithm that implements automatic
failover.

To demonstrate automatic failover, copy the ``launch.py`` script
to ``launch-no-bizlogic.py`` and comment out the bizlogic command.
Then, run the following commands::

  python launch-no-bizlogic.py &
  python bizlogic.py --port 5680 &
  python bizlogic.py --port 5681 &

Then, start the monitoring::

  cd monitoring
  python launch.py &

Load the monitoring UI in your browser. Barks will be routed through
the bizlogic on 5680. If you kill the first bizlogic process, barks
will be routed to the second bizlogic on 5681. You'll see a small,
temporary increase in queue depth when you kill the first bizlogic
process as the failover occurs. If you kill the second bizlogic,
you'll see the queue depth will increase indefinitely. Starting a new
instance of the bizlogic will start queue processing again.

Services and Other Components
=============================

Let's follow the path of a single bark through the system. The
``bark.py`` command line utility lets you submit barks manually.

.. literalinclude:: ../../../barker/bark.py
   :language: python
   :pyobject: PutBark

An instance of the ``Bark`` class contains the username of the sender,
the content being sent, and the bark's unique ID, which in this case is
computed implicitly by the ``Bark`` constructor. The ``Sender`` link is
constructed to point to the outbox of the current user. The bark is sent
as a Python tuple, which Datawire and the underlying Proton Library can
convert to a corresponding AMQP type. The rest of the code is identical
to the ``send`` example.

The ``autobark.py`` utility submits barks the same way, creating random
barks and sending them (as Python tuples) on a regular basis in the
``on_timer_task`` event handler. The web UI's mechanism for sending
barks, implemented in ``webui/barker.js`` in the function
``sendNewBark``, does the same thing.

In each case, the new bark finds its way to the submitting user's
outbox, which is a manifold located at the service address ``//``\
*hostname*\ ``/outbox/``\ *username*\ ``/``. The Barker launch script
launches three manifolds to serve as all the outboxes, with each
manifold pushing messages to the business logic service::

  manifold //%(hostname)s/outbox --port 5800 --push //%(hostname)s/bizlogic
  manifold //%(hostname)s/outbox --port 5801 --push //%(hostname)s/bizlogic
  manifold //%(hostname)s/outbox --port 5802 --push //%(hostname)s/bizlogic

Manifolds use routing based on consistent hashing, so multiple manifolds
on the same base service address receive messages based on the portion
of the address following the base address, which in this case is the
username of the outbox. In other words, barks for a given user will
always be sent to the same outbox manifold regardless of how they are
submitted.

Each manifold is configured to push messages to the business logic,
which is the core user-implemented service in Barker. In ``bizlogic.py``
you can see a service that is much like a combination of the ``printer``
example and the ``send`` example: it acts on received messages in
``on_message``, creating and then using a ``Sender`` to send messages
onward after the appropriate processing is complete.

Let's look at the constructor:

.. literalinclude:: ../../../barker/bizlogic.py
   :language: python
   :pyobject: BizLogic.__init__

The business logic's tether specifies the *ordered* routing policy,
which tells the directory to route messages to the oldest running
process serving on that address, effectively implementing automatic
failover as demonstrated above. The handler is a Datawire ``Container``
that includes the business logic class instance as well as a Datawire
``Agent``. The agent enables monitoring instances of the business logic
service; monitoring is covered in the next section.

The purpose of this service is to consider each bark, determine which
users need to see it, and forward (copies of) it to the associated
inboxes. This actual business logic is expressed in ``on_message``.

.. literalinclude:: ../../../barker/bizlogic.py
   :language: python
   :pyobject: BizLogic.on_message

An instance of ``Bark`` is constructed using the message body, which is
a tuple of the sender's username, the bark content, and the bark's ID.
The code looks for mentions in the bark's textual content, e.g., @Fido.
The final set of target users is the union of every user following the
sender, every user mentioned in the message, and the sender. The code
creates a ``Sender`` for each unique target user's inbox (at address
``//``\ *hostname*\ ``/inbox/``\ *username*\ ``/``) and sends a copy of
the message it received.

The manifold holding recipients' inboxes queues up messages for Barker
clients to receive immediately or retrieve in the future. The manifold
keeps a history of the last 25 barks received, as specified in the
command line invocation::

  manifold //%(hostname)s/inbox --port 5820 --history 25

This bark history allows clients to present some barks to the user
immediately. A more powerful system would keep an archive of every bark
and allow clients to scroll back into the past or search the archive,
but Barker addresses the common case of a user wanting to catch up on
recent barks and watch new barks come in live.

The command line Barker client, ``listen.py``, is completely analogous
to the ``pull`` example. The ``on_message`` code processes barks and
displays them with a bit of formatting:

.. literalinclude:: ../../../barker/listen.py
   :language: python
   :pyobject: GetBarks.on_message

The JavaScript equivalent for the web UI is implemented in
``webui/barker.js`` in the function ``pumpData``. The only interesting
difference is that the JS code performs a sender check so it can
highlight barks sent by the user viewing the incoming stream of barks.


Monitoring and Agents
=====================

The Datawire Monitoring Dashboard presents realtime information about
the state of the microservices in your system. This information is
collected by a Datawire Agent running in the services being monitored.
In our Barker example, the ``bizlogic.py`` service includes the agent.
The ``manifold`` also includes an agent, and manifolds report some
additional information. A service does not have to include an agent; the
tutorial examples do not.

Let's examine the information that the agent provides. Start up Barker
and the Monitoring Dashboard as above, then take a look at the running
services by querying the directory::

  $ dw -d //127.0.0.1/directory route list
  //127.0.0.1/inbox -> (consistent)  on 127.0.0.1:5820
  //127.0.0.1/agents/127.0.0.1-5820 -> (random)  on 127.0.0.1:5820
  //127.0.0.1/bizlogic -> (ordered)  on 127.0.0.1:5680
  //127.0.0.1/agents/127.0.0.1-5680 -> (random)  on 127.0.0.1:5680
  //127.0.0.1/outbox -> (consistent)  on 127.0.0.1:5800;  on 127.0.0.1:5801;  on 127.0.0.1:5802
  //127.0.0.1/agents/127.0.0.1-5800 -> (random)  on 127.0.0.1:5800
  //127.0.0.1/agents/127.0.0.1-5801 -> (random)  on 127.0.0.1:5801
  //127.0.0.1/agents/127.0.0.1-5802 -> (random)  on 127.0.0.1:5802
  //127.0.0.1/monitor -> (random) //127.0.0.1:6000

You'll notice that each running service process has a corresponding
agent service endpoint under ``//``\ *hostname*\ ``/agents/``. An
exception is the ``monitor`` service, which provides the backend for the
Monitoring Dashboard. We can pull messages from one of the agent service
addresses to see what that agent is reporting::

  $ examples/pull //127.0.0.1/agents/127.0.0.1-5680
  Message{body={"incoming_count"=10800, "outgoing_count"=402210, "timestamp"=1433963124427, "pid"=29521, "agent"="//127.0.0.1/agents/127.0.0.1-5680", "times"=[97.64, 13.74, 0, 0, 1.43396e+09], "incoming_rate_lib"=0, "outgoing_count_lib"=1617, "command"=["/usr/bin/python", "bizlogic.py", "--host", "127.0.0.1", "--port", "5680"], "outgoing_rate"=353.897, "incoming_count_lib"=0, "address"="//127.0.0.1/bizlogic", "outgoing_rate_lib"=4.27526, "num_fds"=214, "type"="bizlogic", "rusage"={"ru_nvcsw"=97, "ru_utime"=97.6445, "ru_majflt"=0, "ru_isrss"=0, "ru_nsignals"=0, "ru_nivcsw"=126537, "ru_msgsnd"=410917, "ru_stime"=13.7415, "ru_ixrss"=0, "ru_inblock"=0, "ru_idrss"=0, "ru_maxrss"=27000832, "ru_msgrcv"=408022, "ru_nswap"=0, "ru_oublock"=1, "ru_minflt"=7665}, "incoming_rate"=9.50058}}
  Message{body={"incoming_count"=10810, "outgoing_count"=402569, "timestamp"=1433963125433, "pid"=29521, "agent"="//127.0.0.1/agents/127.0.0.1-5680", "times"=[97.72, 13.75, 0, 0, 1.43396e+09], "incoming_rate_lib"=0, "outgoing_count_lib"=1621, "command"=["/usr/bin/python", "bizlogic.py", "--host", "127.0.0.1", "--port", "5680"], "outgoing_rate"=341.823, "incoming_count_lib"=0, "address"="//127.0.0.1/bizlogic", "outgoing_rate_lib"=4.26687, "num_fds"=214, "type"="bizlogic", "rusage"={"ru_nvcsw"=97, "ru_utime"=97.727, "ru_majflt"=0, "ru_isrss"=0, "ru_nsignals"=0, "ru_nivcsw"=126620, "ru_msgsnd"=411274, "ru_stime"=13.7536, "ru_ixrss"=0, "ru_inblock"=0, "ru_idrss"=0, "ru_maxrss"=27000832, "ru_msgrcv"=408377, "ru_nswap"=0, "ru_oublock"=1, "ru_minflt"=7665}, "incoming_rate"=9.48193}}
  Message{body={"incoming_count"=10820, "outgoing_count"=402967, "timestamp"=1433963126434, "pid"=29521, "agent"="//127.0.0.1/agents/127.0.0.1-5680", "times"=[97.82, 13.76, 0, 0, 1.43396e+09], "incoming_rate_lib"=0, "outgoing_count_lib"=1625, "command"=["/usr/bin/python", "bizlogic.py", "--host", "127.0.0.1", "--port", "5680"], "outgoing_rate"=358.639, "incoming_count_lib"=0, "address"="//127.0.0.1/bizlogic", "outgoing_rate_lib"=4.26387, "num_fds"=214, "type"="bizlogic", "rusage"={"ru_nvcsw"=97, "ru_utime"=97.8211, "ru_majflt"=0, "ru_isrss"=0, "ru_nsignals"=0, "ru_nivcsw"=126738, "ru_msgsnd"=411687, "ru_stime"=13.7664, "ru_ixrss"=0, "ru_inblock"=0, "ru_idrss"=0, "ru_maxrss"=27000832, "ru_msgrcv"=408787, "ru_nswap"=0, "ru_oublock"=1, "ru_minflt"=7665}, "incoming_rate"=9.47527}}
  ^C

The output is a map of string keys to values updating roughly once per
second. The ``command`` entry shows this agent is associated with
``bizlogic.py``; the ``address`` entry contains the service address of
this process. There are a number of other keys that convey basic process
information and statistics pulled from the underlying operating system.
The most noteworthy messaging-specific keys are ``incoming_rate`` and
``outgoing_rate``, which convey (in messages per second) the rate of
message flow through the service over the last ten seconds.

Manifolds have some additional fields. Noteworthy are
``manifold_messages``, which is the total number of messages being
tracked by the manifold at that moment, ``manifold_streams``, which is
the number of distinct service addresses currently holding messages or
with an active subscriber, and ``manifold_depth``, which is the maximum
depth of any individual service address at that moment.

The Monitoring Dashboard web-based UI uses the ``monitoring/monitor.py``
service as its back-end. The monitor service subscribes to the directory
to be aware of running agents. It then subscribes to every agent and
collects their reports. Once per second, it pushes a message onto its
internal stream containing a list of those agent messages.

If you pull from the monitor, you will see those combined report
messages. The web UI does essentially the same thing. It pulls from the
monitor and receives one message per second containing a collection of
reports from all known agents. It then plots message rates for all
services on the lower chart and all manifold sizes on the upper chart.
Note that at present the UI explicitly skips Barker's inbox manifold(s)
to keep the graph y-axis scales manageable.


.. _websockets-notes:

Browser-based UI Notes
======================

Pointing the browser directly at the local filesystem is generally the
easiest way to access the Barker UI and Monitoring Dashboard web pages.
Simply use your browser's "Open File..." menu and navigate to the
``index.html`` file in the appropriate directory. Or you can type in the
URL directly: ``file://`` followed by the absolute path to the
``index.html`` file.

Barker relies on the ``proton.js`` library to speak AMQP over WebSockets
and a Node.js tool that proxies between WebSockets and TCP. This allows
the Barker UI and the Monitoring Dashboard to communicate with the
Python code without having to use an intervening web server and dealing
with the request-response nature of HTTP. Note that the ``proton.js``
library should be considered experimental, as the Datawire-style
reactive API is not yet implemented in ``proton.js``.

WebSockets are not restricted by an origin policy; browser Javascript
loaded from one host may connect to a different host. As a result, the
Barker and Monitoring launch scripts can pass the specified hostname to
the JS code on the filesystem, allowing the web page to talk to the rest
of the system directly as a first-class peer regardless of where the
browser is running. Furthermore, unlike a typical XMLHTTPRequest-based
implementation, no web server is required. The page's JS is able to
connect to the network even if the browser accesses the web page using a
``file://`` URL as described above.

Use of a web browser and the WebSocket Proxy can add an unexpected
complication: DNS resolution sometimes works differently across the
different pieces (the Python/Datawire code, the browser, and the proxy).
In particular, we have noticed that the hostname ``localhost`` can
resolve to two different addresses on machines with IPV6 enabled (i.e.
most modern systems). To avoid this problem, the default hostname for
Barker and the Monitoring Dashboard is ``127.0.0.1``. Please keep this
issue in mind if you pass a hostname to the launchers.
